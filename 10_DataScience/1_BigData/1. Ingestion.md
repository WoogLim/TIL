## Ingestion
---
- 능동적으로 데이터를 가져오는 행위
  
1. 데이터 입수 방식
- 어떻게 데이터를 입수할 것인가?
2. 데이터 입수 형태
- 어떤 데이터가 들어오는가?

## 1. 데이터 입수 방식
---
### [1] 배치 처리 방식
 - 일정 시간에 정기적으로 일정 데이터를 일정 방식에 의해 일정 스토리지에 저장하는 방식
 - 목적은 어제 일어난 일들을 오늘 알고 싶어할때 활용(ex 사용자들의 activity)

### [장점]
- 운영하기에 용이함. 문제가 발생하면 대부분이 경우 배치를 임의 조정할 수 있음.
- 데이터가 흘러가기 때문에 이를 붙잡아두기 힘든데 데이터 플랫폼에서 배치를 조정할 수 있다. 단 이 과정에서 장애는 발생한다.
### [단점]
- 배치 시간을 극단적으로 작게(1초, 5분) 장애처리가 끝나지 않기 때문에 다음에 배치가 들어오면 이를 해결하기에 작업이 많이 든다.
- 빠른 분석이 필요한 경우, 별도의 장치를 마련해야한다.
최근에는 빠른 분석을 필요로 하는 경우가 많아짐.
- 배치는 빠른 시간에 확인하기에 적합하지 않음.

### [2] Ad-hoc
- 예상할 수 없고 비정기적으로 기존에는 쌓지 않던 데이터를 저장하는 경우 사용하는 입수 방식이다.
- 주로 DB덤프나 사용자 정보를 한번에 입수하는 경우(비식별화된 사용자 정보) 특정 사용자 혹은 그룹에 대한 요구사항이 있을때 비정기적으로 업로드 하는데 쓰임. 단 작업 후 해당 정보는 반드시 지운다.
(비식별화 - 트랜잭션은 남지만 이)

### [장점]
- 여러번 해도 상관이 없고 실패해도 다시 해당 패스만 지우고 업데이트 하거나 테이블을 날리고 다시 입력하기 때문에 대부분 멱등성이 보장된다.

### [단점]
- csv, binary, txt 등 저장용도 및 형식 저장해야할 데이터가 매번 바뀌어 새로운 방식을 요구
- 정해져있지 않은 입수 방식, 저장 방식이여서 일회성으로 주로 이용한다.
- 높은 데이터 정합성이 요구되는 경우가 많다.(무조건 마지막 결과는 Input과 똑같아야 함.) binary를 csv로(혹은 다른 방식) 변경 후 데이터를 처리해 mongoDB에 저장해주세요 등 .. 정합성을 맞추기에 힘듦.

### [3] (근)실시간
- 요즘 로그는(워낙 많아서) 일일히 살펴보기 힘들고 장애부분 파악이 힘들다. 정상적인 부분도 입수하기 힘듦
- 장애 대응이나, 고객응대를 위한 데이터인 경우 이용
- 실시간으로 고객의 반응을 보고 싶은 경우 사용
- 앞 두 방식에 비해 장점이 많다.

### [장점]
- 서비스 지연시간이 적다.
- 데이터 발생, 입수 시점에서 서비스하는데 쓸 용도인 경우 많은 시간이 절약됨
- 실시간 분석에 가장 용이함.

### [단점]
- 장애 대응에 아주 어렵다. 주로 서비스에 직접 노출된 경우.
- 유실, 중복의 가능성이 존재함.
- 장애가 발생하는 시점에 이후 데이터는 유실, 중복의 가능성이 있음
- 서비스 단계에 장애로 번질 위험이 있음. 서비스가 장애로 인식하지 않도록 하는 문제가 가장 큰 문제로 요구됨
- API 레이어를 따로 요구하는 방식을 이용하기도 함.

## 2. 데이터 입수 형태
---
### [1] 정형 데이터
- 빅데이터 이전 시절부터 많이 활용되어 비교적 정리되어 있는 데이터(ex.RDB 데이터 베이스 형태) 테이블 형태로 저장할 목적으로 남긴 데이터
- 분석을 용이하게 하기 위해 데이터 정의에 시간을 많이 쏟음
- 하나의 서비스의 생명주기가 긴 경우 주로 사용됨.
- 목적이 있는 로그 데이터. 게임 로그, 앱 다운로드 로그, 구매 로그
- 배치로 입수하기에 유리함.

### [2] 비정형 데이터
- 아무런 정의가 없는 데이터
- 어떤 데이터가 필요한지 처음부터 알 수 없는 경우가 대부분.
- 일단 데이터를 남기고 싶은 욕구로 마구잡이로 남긴 후 그 안에서 목적을 찾고자하는 데이터들
- 저장하는 데이터의 형식이 변경되는 경우 및 내용이 변경되는 경우도 있음
- 1의 의미가 성인에서 아이로 변경되는 경우
- 즉흥적으로 추가되는 경우가 많음. 서버 운영 로그, 앱 장애 대응용 클라이언트 로그 등
- 새로운 패턴의 데이터가 제공되면서 장애로 번지는 경우 어떤 함수의 분기에 내재해 해당 로그를
저장하기도 함.

## 3. OSS for Ingestion
---
### [1] Apache Sqoop
- 초기에 사용하던 배치
- 커맨드 라인 형태의 배치를 지원
- 서비스의 주요 DB를 Snapshot의 형태로 저장하는 용도
- JDBC를 지원한느 대부분의 DB에서 사용 RDB의 데이터를 HDFS로 복사하기 위해 사용한다.
- 서비스 DB에 부하를 줄이며, 분석을 용이하게 하기 위해 사용
- 기술적으로는 MapReduce와 같이 동작함. Mapper만 사용. 해당 Mapper가 실패하면 Retry 구문으로 대부분의 장애를 해결함.
- 대부분 MR에서 알아서 실행해 줌.
- apache spark에서 동일한 작업이 가능하다.

### [2] Apache Kafka
- 실시간 데이터 수집을 위해 주로 사용. 실제 서비스에서 메시지큐 및 MS아키텍쳐를 활용하는데 쓰기 때문에 압도적으로 많이 사용된다.
- 스트림 형태로 이루어진 모든 데이터를 처리 가능. 배치도 보내는 쪽에서 스트림으로 쪼개서 보내면 처리 가능 binary, json, image 등 청크 단위로 들어오는 스트림이라면 모두 처리 가능하다.
- batch 쪽에서 batch를 사용하기 위해 스트림으로 만들어 kafka에 적재하면 batch 작동이 가능하다.
- 배치와 실시간 수집을 위해 Kafka를 사용하고 배치가 너무 크다면 이부분에 대해선 고려해야 하지만 작동은 가능하다.
- 대용량 스트림 데이터를 다루는데 최적화 되어 있음. 메시지큐는 주로 복잡한 형태의 재시도 방식을 제공하는 것이 주된 목표.(Kafka사용시 초당 수십, 수십만건 처리가 가능하다.) 받은 데이터를 큐에 저장.
- 정상적으로 응답을 받은 경우 유실은 걱정하지 않아도 됨. 활용도에 따라 중복으로 데이터가 전송되는 경우는 있다.
- 경로를 설정해 다양한 형태의 메타데이터 지원. 데이터 자체를 변경하지 않을 수 있다.
- 파티션 키를 통해 순서를 보장할 수 있으며 일반적으로 순서를 보장하지 않고 성능에 초점을 맞춤
- 주문시스템 등 주문 하려는 물품이 있는지 확인 후 수량 선택 결제를 누르고 취소가 일어난 경우 시간 순서가 굉장히 중요한데 이런 경우 파티션키를 이용해 어느정도 순서를 보장할 수는 있다.

### [3] Fluentd
- 주로 로그 데이터를 수집하는데 쓰임
- 파일로 남겨진 로그 데이터를 처리하는 일은 생각보다 복잡하다. 파일 이름 규칙이 변경되고 위치가 변경되며 특정 파일이 깨져있는 경우가 있음.
- Cloud Native Computing Foundation에서 제공
- 정형화된 로그 형태들이 존재. Syslog, Apache/Nginx logs 등. 포맷들이 지정된 경우 사용하기에 적합하다.
- 알람, 분석, 저장을 별도 추가 모듈 없이 지원한다.
- 카프카는 데이터를 저장하기 위해 서비스내에 심어서 배포해야 하지만 플루언트는 기존 서비스의 로그파일만 저장해두면 플루언티드에서 읽기만 하면 되기 때문에 서비스에 영향을 미칠 영향이 적고 용이하다.